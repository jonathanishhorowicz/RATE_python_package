{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# The import statements have changed - now need to import each part of the RATE calculation separately\n",
    "from rate.models import BnnBinaryClassifier # The BNN model\n",
    "from rate.projections import CovarianceProjection # The projection operator (can also use Pseudoinverse projection)\n",
    "from rate.cross_validation import cross_validate_bnn # To cross-validate a BNN\n",
    "from rate.importance import RATE2 # To calculate RATE values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of changes\n",
    "\n",
    "I have rewritten lots of the code (and will push it to Lorin's branch once we submit).\n",
    "\n",
    "Most of the code is documented so look there first if you have any problems, but email me if something is not working. I have tried to put argument checks in to help explain the most common errors.\n",
    "\n",
    "### The BNN models\n",
    "\n",
    "The main changes are that the BNN models have been coded to match the scikit-learn API, so they have methods like\n",
    "* fit\n",
    "* predict\n",
    "* predict_proba\n",
    "* score\n",
    "\n",
    "None of the model functions will work until fit is called (that is when the model actually gets constructed).\n",
    "\n",
    "Another change is that the logit posterior calculation is now a method of the BNN model itself, so you calculate it using something like\n",
    "\n",
    "`bnn = BnnBinaryClassifier()`\n",
    "\n",
    "`bnn.fit(X_train, y_train)`\n",
    "\n",
    "`M_F, V_F = bnn.logit_posterior(X_test)`\n",
    "\n",
    "All these functions are documented in `models.py`.\n",
    "\n",
    "### RATE calculation\n",
    "\n",
    "This is now done by the `RATE2` function in `importance.py`. Pass it the logit posterior that you will already have comptued using the class method for the bnn. \n",
    "\n",
    "This isn't the `RATE_V2` function you were working on a while ago, I just called it this to distinguish it from another older function in the Python code.\n",
    "\n",
    "The projections are also implemented as their own objects: `CovarianceProjection` and `PseudoinverseProjection`.\n",
    "\n",
    "They are defined in `projections.py` and inherit from the `ProjectionBase` abstract class. You can define new projections - they just need a method called `esa_posterior` that calculates the effect size analogue posterior from a set of examples and a logit posteiror. This method is called internally by `RATE2` and it uses the `CovarianceProjection` by default.\n",
    "\n",
    "### Mimic models\n",
    "\n",
    "I have added a couple of arguments to `train_mimic` and changed how it works internally.\n",
    "\n",
    "The arguments are now \n",
    "\n",
    "`train_mimic(mimic_model, bnn, x_train, y_train=None, x_test=None, n_mc_samples=100, return_time=False)`\n",
    "\n",
    "The `mimic_model` can be any sklearn method that implements `fit`, so this includes any estimator (e.g. `RandomForestRegressor`) but also cross-validation objects (e.g. `RandomizedSearchCV`), meaning that you can\n",
    "easily use a pre-specified mimic model or perform cross-validation. The returned mimic model is of the same type \n",
    "as the `mimic_model` passed as an argument (e.g. if you pass a `RandomizedSearchCV` you will get one back and then have to extract the model itsef from the `RandomizedSearchCV` object).\n",
    "\n",
    "The `y_train` argument is useful if you want to train several mimic models on the same predictions - by default it is `None` but if you pass some labels the `bnn` will be ignored and they will be used to train the mimic model. Otherwise (if `y_train` is `None`) the `bnn` is used to generate the predictions.\n",
    "\n",
    "Finally, whatever estimator/cross-validation object you choose has to be for regression - this is becuase the mimic models are trained on the prediction probabilities (not the class labels) as they contain more information.\n",
    "\n",
    "### Cross validation\n",
    "\n",
    "I added a function that does stratified K-fold cross validation for the BNN. Pass it a list of layers, `X`, `y` and the number of folds and it will do cross-validation and return the best model. The type of model (regression or classification) is inferred from the type of labels in `y`.\n",
    "\n",
    "There are other arguments where you pass 'keyword' : 'value' dictionaries to the BNN __init__, fit and score methods.\n",
    "\n",
    "### Logging\n",
    "\n",
    "If you put\n",
    "\n",
    "`import logging\n",
    "logging.getLogger('rate').setLevel('INFO')`\n",
    "\n",
    "at the start of your script the logger should tell you useful information about the computation. It is not fully working yet - it often prints the same things multiple times - but it should stil be useful. You could also use `setLevel('DEBUG')` if you want to have loads of information of dubious relevance printed.\n",
    "\n",
    "There is also a progress bar for the RATE calculation but I haven't managed to get it to work properly yet either - sometimes it behaves a bit strangely and I don't know why - so if you notice any strange behaviour please let me know!\n",
    "\n",
    "# Example Code\n",
    "\n",
    "The code below demonstrates all of this.\n",
    "\n",
    "Let me know if any of it doesn't work/isn't clear!\n",
    "\n",
    "### Running the local methods\n",
    "\n",
    "For the local methods you need a standard neural network that is equivalent to the Bayesian neural network we normally use for RATE.\n",
    "\n",
    "The function `get_deterministic_nn` does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deterministic_nn(bnn):\n",
    "    \"\"\"Returns a deterministic neural network with the same architecture as \n",
    "    a given Bayesian neural network. Binary classification only\"\"\"\n",
    "    if not isinstance(bnn, BnnBinaryClassifier):\n",
    "        raise ValueError(\"get_deterministic_nn is only for BnnBinaryClassifiers\")\n",
    "    nn = Sequential()\n",
    "    for layer in bnn._logit_model.layers[:-1]:\n",
    "        deepcopied_layer = layer.__class__.from_config(layer.get_config())\n",
    "        if hasattr(deepcopied_layer, 'kernel_initializer'):\n",
    "            deepcopied_layer.build(layer.input_shape)\n",
    "            deepcopied_layer.kernel.initializer.run(session=K.get_session()) # To reinitialise the weights\n",
    "        nn.add(deepcopied_layer)\n",
    "    nn.add(Dense(bnn._logit_model.layers[-1].output_shape[1]))\n",
    "    nn.add(tf.keras.layers.Activation(activation=\"sigmoid\"))\n",
    "    nn.compile(loss=\"binary_crossentropy\", optimizer=\"Adam\", metrics=[\"accuracy\"])\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load packages\n",
    "#\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "from rate.models import BnnBinaryClassifier\n",
    "from rate.projections import CovarianceProjection\n",
    "from rate.mimic import train_mimic\n",
    "from rate.importance import RATE2\n",
    "\n",
    "from deepexplain.tensorflow import DeepExplain # Get this from https://github.com/marcoancona/DeepExplain\n",
    "\n",
    "import logging\n",
    "logging.getLogger('rate').setLevel('INFO') # Change to 'DEBUG' for a lot more messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of class 1: 592,\tClass 2:408\n",
      "WARNING:tensorflow:From /home/jonathan/anaconda3/envs/rate-bnn/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonathan/anaconda3/envs/rate-bnn/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonathan/anaconda3/envs/rate-bnn/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jonathan/anaconda3/envs/rate-bnn/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNN test score is 0.9633333333333334\n",
      "300/300 [==============================] - 0s 164us/sample - loss: 0.1924 - acc: 0.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:rate.mimic:Using provided y_train and ignoring the supplied BNN for mimic training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN test score is 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rate.mimic:Mimic R^2 on x_test: 0.946\n",
      "WARNING:rate.mimic:Using provided y_train and ignoring the supplied BNN for mimic training\n",
      "/home/jonathan/anaconda3/envs/rate-bnn/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "INFO:rate.mimic:Mimic R^2 on x_test: 0.926\n",
      "WARNING:rate.mimic:Using provided y_train and ignoring the supplied BNN for mimic training\n",
      "INFO:rate.mimic:Mimic R^2 on x_test: 0.946\n",
      "INFO:rate.importance:Calculating RATE values for 1 classes, 300 examples and 10 variables\n",
      "INFO:rate.importance:Calculating RATE values for class 1 of 1\n",
      "100%|██████████| 10/10 [00:00<00:00, 2704.43it/s]\n",
      "INFO:rate.importance:The RATE calculation took 0.008 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating RATE values for 1 classes, 300 examples and 10 variables\n",
      "Calculating RATE values for class 1 of 1\n",
      "The RATE calculation took 0.008 seconds\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Simulate the data and labels (linearly)\n",
    "#\n",
    "n = 1000\n",
    "p = 3\n",
    "p_decoy = 7\n",
    "test_size = 0.3 # Size of test set\n",
    "n_mc_samples = 10\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "\n",
    "#\n",
    "# Cross-validation settings for mimic models\n",
    "#\n",
    "n_search_iter = 5\n",
    "k = 3\n",
    "n_jobs = 7\n",
    "\n",
    "# Random forest CV grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators' : np.arange(10, 1000, 10),\n",
    "    'max_depth' : np.arange(1, p + p_decoy),\n",
    "    'max_features' : ['auto', 'log2', 'sqrt']\n",
    "}\n",
    "\n",
    "# Gradient boosting machine CV grid\n",
    "gbm_param_grid = {\n",
    "    'learning_rate' : [10.0**x for x in [-3.0, -2.0, -1.0]],\n",
    "    'subsample' : [0.5, 0.7, 0.9, 1.0],\n",
    "    'n_estimators' : np.arange(10, 1000, 10),\n",
    "    'max_depth' : np.arange(1, p + p_decoy),\n",
    "    'max_features' : ['auto', 'log2', 'sqrt']\n",
    "}\n",
    "\n",
    "#\n",
    "# Simulation starts here\n",
    "#\n",
    "X = np.random.randn(n, p)\n",
    "\n",
    "effect_sizes = np.random.randn(X.shape[1])\n",
    "f = np.dot(X, effect_sizes)\n",
    "y = (f > np.quantile(f, np.random.uniform(0.4, 0.6))).astype(int)[:,np.newaxis] # Class imbalance will be at worst 60/40\n",
    "\n",
    "# Add decoy variables\n",
    "X = np.hstack([X, np.random.randn(n, p_decoy)])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\n",
    "\n",
    "print(\"Number of class 1: {},\\tClass 2:{}\".format(y.shape[0]-np.sum(y), np.sum(y))) # Don't want too much of a class imbalance\n",
    "\n",
    "#\n",
    "# Fit the models and evaluate their test accuracy\n",
    "#\n",
    "\n",
    "# BNN\n",
    "bnn = BnnBinaryClassifier(verbose=0, n_mc_samples=n_mc_samples)\n",
    "bnn.fit(X_train, y_train, epochs=10) # Pass kwargs to control network training\n",
    "print(\"BNN test score is\", bnn.score(X_test, y_test, metric=\"accuracy\"))\n",
    "\n",
    "nn = get_deterministic_nn(bnn)\n",
    "nn.fit(X_train, y_train, verbose=0, epochs=10) # Pass kwargs to control network training\n",
    "print(\"NN test score is\", nn.evaluate(X_test, y_test)[1])\n",
    "\n",
    "#\n",
    "# Variable importance\n",
    "#\n",
    "\n",
    "# Mimic models (using cross validation)\n",
    "bnn_soft_predictions = bnn.predict_proba(X_train)\n",
    "rf_mimic, rf_mimic_time = train_mimic(\n",
    "    RandomizedSearchCV(\n",
    "        RandomForestRegressor(),\n",
    "        rf_param_grid,\n",
    "        n_iter=n_search_iter,\n",
    "        cv=k,\n",
    "        n_jobs=n_jobs),\n",
    "    bnn, X_train, bnn_soft_predictions, X_test, n_mc_samples, True\n",
    ")\n",
    "\n",
    "# The same with no cross-validation\n",
    "rf_mimic, rf_mimic_time = train_mimic(\n",
    "            RandomForestRegressor(), bnn, X_train, bnn_soft_predictions, X_test, n_mc_samples, True\n",
    ")\n",
    "\n",
    "gbm_mimic, gbm_mimic_time = train_mimic(\n",
    "    RandomizedSearchCV(\n",
    "        GradientBoostingRegressor(),\n",
    "        gbm_param_grid,\n",
    "        n_iter=n_search_iter,\n",
    "        cv=k,\n",
    "        n_jobs=n_jobs),\n",
    "    bnn, X_train, bnn_soft_predictions, X_test, n_mc_samples, True\n",
    ")\n",
    "\n",
    "# RATE\n",
    "M_F, V_F = bnn.logit_posterior(X_test)\n",
    "rate_vals, rate_time = RATE2(X_test, M_F, V_F, projection=CovarianceProjection(), return_time=True)\n",
    "\n",
    "# Saliency-style maps, averaged over examples to give global importance. Average is of absolute value\n",
    "# This just calculates them and doesn't do anything with them\n",
    "with DeepExplain(session=K.get_session()) as de:\n",
    "    input_tensor = nn.layers[0].input\n",
    "    target_tensor = Model(inputs=input_tensor, outputs=nn.layers[-2].output)(input_tensor)\n",
    "\n",
    "    for attr_method in [\"grad*input\", \"saliency\", \"intgrad\", \"elrp\", \"occlusion\"]:\n",
    "        imp_vals = de.explain(\n",
    "            attr_method,\n",
    "            target_tensor, input_tensor,\n",
    "            X_test, ys=y_test, # This needs 2D arrays so you may have to add an axis here if using 1D labels\n",
    "            batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rate-bnn",
   "language": "python",
   "name": "rate-bnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
